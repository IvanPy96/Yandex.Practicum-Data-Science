{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYNfHu8aR9lY",
    "outputId": "c1af0236-bb84-4136-ceda-7024861e3fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 21 14:26:49 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   73C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETvdi04escnB"
   },
   "source": [
    "В нашем распоряжении видеокарта Tesla K80. Неплохо! Используем ее на всю, так сказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QlRFXNeA7GUP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfGpsRdX8hsy",
    "outputId": "a09c7c8a-f3b8-40f2-dbec-0fae20b17f62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/content/toxic_comments.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eI-9M9CoGQKu"
   },
   "outputs": [],
   "source": [
    "# Функция для очистки текста и приведения к нижнему регистру\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = text.lower()    \n",
    "    text = re.sub(r\"(?:\\n|\\r)\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z ]+\", \"\", text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "XAUpcErDGcRg",
    "outputId": "94c88726-58e6-4ff4-d79d-782a6a43aa2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55936</th>\n",
       "      <td>Green Tea and CigarettesWhy you're a worthless...</td>\n",
       "      <td>green tea and cigaretteswhy youre a worthless ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142392</th>\n",
       "      <td>\"\\n\\n Non-enyclopedic? \\n\\n\"\"On February 24, 1...</td>\n",
       "      <td>nonenyclopedic   on february   iowa put to sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118335</th>\n",
       "      <td>\"\\nHi, the page that I wanted deleting was the...</td>\n",
       "      <td>hi the page that i wanted deleting was the pag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130840</th>\n",
       "      <td>Well the article i created then smart ass. why...</td>\n",
       "      <td>well the article i created then smart ass why ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81788</th>\n",
       "      <td>\"\\n\\nSpeedy deletion of Www.wikipedia.com\\n A ...</td>\n",
       "      <td>speedy deletion of wwwwikipediacom  a tag has ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text                                         text_clean\n",
       "55936   Green Tea and CigarettesWhy you're a worthless...  green tea and cigaretteswhy youre a worthless ...\n",
       "142392  \"\\n\\n Non-enyclopedic? \\n\\n\"\"On February 24, 1...  nonenyclopedic   on february   iowa put to sea...\n",
       "118335  \"\\nHi, the page that I wanted deleting was the...  hi the page that i wanted deleting was the pag...\n",
       "130840  Well the article i created then smart ass. why...  well the article i created then smart ass why ...\n",
       "81788   \"\\n\\nSpeedy deletion of Www.wikipedia.com\\n A ...  speedy deletion of wwwwikipediacom  a tag has ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_clean'] = data['text'].apply(clean)\n",
    "data[['text', 'text_clean']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxZLpOcHPZBe",
    "outputId": "17e675a4-10eb-4222-9c8c-5f6a25c29275"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 25/25 [1:12:37<00:00, 174.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7241379310344829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')  # Создаем токенизатор\n",
    "tokenized = data['text'][:2500].apply(lambda x: tokenizer.encode(x,truncation=True, add_special_tokens=True)) #Производим токенизацию текстов в датасете\n",
    "\n",
    "max_len = 0  #Находим максимальную длину токенизированного комментария\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "# Создаем матрицу размера max_len x n_tokens (размер максимального токенизированного комментария на кол-во текстов) и маску\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "# Используем заранее обученную BERT-модель\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Задаем размер батча, т.е. такое кол-во комментариев, которое будет принимать на вход модель\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(padded.shape[0] // batch_size)):\n",
    "        # проходим по batch_size, 0...100, 100....200, и т.д. то есть берем по кусочку матриц\n",
    "        # padded и attention_mask и оборачиваем это в тензоры\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        # no_grad = без обучения\n",
    "        with torch.no_grad():\n",
    "            # получаем готовый эмбеддинг по батчу и маске\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        # добавляем полученный эмбеддинг в список, переведя его в массив numpy\n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "\n",
    "# Склеиваем полученные значения в один массив numpy\n",
    "features = np.concatenate(embeddings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(features), \n",
    "                                                    data['toxic'][:2500],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(f1_score(pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
